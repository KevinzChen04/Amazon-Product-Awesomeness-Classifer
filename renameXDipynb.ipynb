{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L890G_Xkcl6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "1fe1c7b9-183a-4f9e-a85b-36eb091e12d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2fde001848fc>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#connect to google drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/My Drive/devided_dataset_v2/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#import and clean data\n",
        "#importing packages\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import sklearn as skl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "import collections\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import math\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#connect to google drive\n",
        "drive.mount('/content/drive/')\n",
        "path=\"/content/drive/My Drive/devided_dataset_v2/\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/data.csv\n",
        "\n",
        "#read data from google drive\n",
        "#train = pd.read_json('/content/drive/My Drive/devided_dataset_v2/CDs_and_Vinyl/train/review_training.json')\n",
        "#train = pd.read_json('/content/drive/My Drive/devided_dataset_v2/CDs_and_Vinyl/test2/review_test.json')\n",
        "awesometrain = pd.read_json('/content/drive/My Drive/devided_dataset_v2/CDs_and_Vinyl/train/product_training.json')\n",
        "\n",
        "parsedData = pd.read_json('/content/drive/My Drive/devided_dataset_v2/vaderHistogramStandardized.json')\n",
        "parsedTest = pd.read_json('/content/drive/My Drive/devided_dataset_v2/vaderHistogramStandardizedTest.json')\n",
        "\n",
        "#test = pd.read_json('/content/drive/My Drive/devided_dataset_v2/CDs_and_Vinyl/test1/review_test.json')\n",
        "#awesometest = pd.read_json('/content/drive/My Drive/devided_dataset_v2/CDs_and_Vinyl/test1/product_test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za7I3OXkanQp"
      },
      "outputs": [],
      "source": [
        "trainData = pd.DataFrame(parsedData)\n",
        "#trainData = trainData.drop(['reviews', 'summaries', 'asin'], axis=1)\n",
        "testData = pd.DataFrame(parsedTest)\n",
        "#testData = testData.drop(['reviews', 'summaries', 'asin'], axis=1)\n",
        "awesometrain = awesometrain.drop(\"asin\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cnwknSK0klr"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(train)\n",
        "df['unixReviewTime'] = pd.to_numeric(df['unixReviewTime'])\n",
        "df = df.sort_values('asin')\n",
        "\n",
        "#test.columns = ['asin', 'reviewerID', 'unixReviewTime', 'vote', 'verified', 'reviewTime', 'style', 'reviewerName', 'reviewText', 'summary', 'image']\n",
        "#test = test.drop(labels=0, axis=0)\n",
        "#dftest = pd.DataFrame(test)\n",
        "#dftest['unixReviewTime'] = pd.to_numeric(dftest['unixReviewTime'])\n",
        "#dftest = dftest.sort_values('asin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWIsEB60mKEh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "2deb0962-4d54-46ac-e388-02ab8560e77a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       awesomeness\n",
              "0                1\n",
              "1                0\n",
              "2                0\n",
              "3                1\n",
              "4                0\n",
              "...            ...\n",
              "73077            1\n",
              "73078            1\n",
              "73079            0\n",
              "73080            1\n",
              "73081            1\n",
              "\n",
              "[73082 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2912e53b-0f23-49ac-ae84-446ee979c887\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>awesomeness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73077</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73078</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73079</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73080</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73081</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73082 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2912e53b-0f23-49ac-ae84-446ee979c887')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2912e53b-0f23-49ac-ae84-446ee979c887 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2912e53b-0f23-49ac-ae84-446ee979c887');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "awesometrain = awesometrain.drop(\"asin\", axis=1)\n",
        "awesometrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh9Uy_TAzeJB"
      },
      "outputs": [],
      "source": [
        "def sentimentalAnalysis(df):\n",
        "  # initialize varaibles\n",
        "  sol = np.zeros([2, df.shape[0]])\n",
        "  sentanal = SentimentIntensityAnalyzer()\n",
        "\n",
        "  # loop through all reviews\n",
        "  for i in range(df.shape[0]):\n",
        "\n",
        "    # if text is empty, then sentimental analysis=0\n",
        "    if not df['reviewText'].iloc[i]:\n",
        "      sol[0][i] = 0;\n",
        "    else:\n",
        "      #otherwise, calculate Sentimental analysis values\n",
        "      SAValue = sentanal.polarity_scores(df['reviewText'].iloc[i])\n",
        "      sol[0][i] = SAValue[\"compound\"]\n",
        "\n",
        "    #do sent anal on summary text\n",
        "    if not df['summary'].iloc[i]:\n",
        "      sol[1][i] = 0;\n",
        "    else:\n",
        "      #otherwise, calculate Sentimental analysis values\n",
        "      SAValue = sentanal.polarity_scores(df['summary'].iloc[i])\n",
        "      sol[1][i] = SAValue[\"compound\"]\n",
        "  return sol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEbI_8rb1Acm"
      },
      "outputs": [],
      "source": [
        "def uniqueItems(df):\n",
        "  #initialize Variables\n",
        "  uniqueItems = 1\n",
        "  curritem = df['asin'].iloc[0]\n",
        "\n",
        "  #loop through all rows\n",
        "  for i in range(df.shape[0]):\n",
        "\n",
        "    #check if the review is unique\n",
        "    if df['asin'].iloc[i] != curritem:\n",
        "      curritem = df['asin'].iloc[i]\n",
        "      uniqueItems = uniqueItems + 1\n",
        "      #end of if statement\n",
        "\n",
        "  return uniqueItems\n",
        "\n",
        "#unique = uniqueItems(trainData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7IOXxRaBKrR"
      },
      "outputs": [],
      "source": [
        "#parse data\n",
        "def parseRow(votes, unixReview, reviewCount, nones, verified, compound,\n",
        "             compoundsumm, reviewlens, summarylens, reviews, summaries, asin):\n",
        "      row = np.zeros(57, dtype=object)\n",
        "\n",
        "      #Percent of Nones final Tally\n",
        "      row[0] = nones / reviewCount\n",
        "\n",
        "      #Max Votes\n",
        "      row[1] = np.max(votes)\n",
        "\n",
        "      #Average Vote\n",
        "      row[2] = np.average(votes)\n",
        "\n",
        "      #Standard deviation votes\n",
        "      row[3] = np.std(votes)\n",
        "\n",
        "      #Percent of verified\n",
        "      row[4] = np.sum(verified)/reviewCount\n",
        "\n",
        "      #Minimum Review Time\n",
        "      row[5] = np.min(unixReview)\n",
        "\n",
        "      #Maximum Review Time\n",
        "      row[6] = np.max(unixReview)\n",
        "\n",
        "      #Average review time\n",
        "      row[7] = np.average(unixReview)\n",
        "\n",
        "      #Standard Deviation of review time\n",
        "      row[8] = np.std(unixReview)\n",
        "\n",
        "      #Number of reviews\n",
        "      row[9] = reviewCount\n",
        "\n",
        "      #sentimental analysis\n",
        "      row[10] = np.average(compound)\n",
        "      row[11] = np.std(compound)\n",
        "\n",
        "      #next 18 are sentiment histogram\n",
        "      #next 18 after that are votes counter histogram\n",
        "      for i in range(len(compound)):\n",
        "        #split compound into histogram\n",
        "        index = 12\n",
        "        if compound[i] != 0:\n",
        "          index = index + math.ceil((compound[i]*4)+4)\n",
        "        if verified[i] == 0:\n",
        "          index = index + 9\n",
        "        row[index] = row[index] + 1\n",
        "\n",
        "        #count the votes\n",
        "        index = index + 18\n",
        "        row[index] = row[index] + votes[i]\n",
        "\n",
        "      #turn the compound sentanal into percentage\n",
        "      for i in range(12, 30):\n",
        "        row[i] = row[i]/len(compound)\n",
        "\n",
        "      row[48] = np.average(compoundsumm)\n",
        "      row[49] = np.std(compoundsumm)\n",
        "\n",
        "      #length of review and summary\n",
        "      row[50] = np.average(reviewlens)\n",
        "      row[51] = np.average(summarylens)\n",
        "      row[52] = np.std(reviewlens)\n",
        "      row[53] = np.std(summarylens)\n",
        "\n",
        "      #review and summary raw text\n",
        "      row[54] = ''\n",
        "      row[55] = ''\n",
        "      for i in range(len(reviews)):\n",
        "        if reviews[i] != None:\n",
        "          row[54] = row[54] + reviews[i] + ' '\n",
        "        if summaries[i] != None:\n",
        "          row[55] = row[55] + summaries[i] + ' '\n",
        "\n",
        "      #asin\n",
        "      row[56] = asin\n",
        "\n",
        "      return row\n",
        "\n",
        "def parseData(unique, SA, df):\n",
        "  #add rows and columns to aggregated data\n",
        "  newData = pd.DataFrame(columns=[\"Percent of Nones\", \"Max Votes\", \"Average Votes\",\n",
        "                                  \"Standard Deviation Votes\", \"Percent of Verified\",\n",
        "                                  \"Minimum Review Time\", \"Maximum Review Time\",\n",
        "                                  \"Average Review Time\", \"Stand Deviation of Review Time\",\n",
        "                                  \"Number of Reviews\", \"compound average\", \"compound SD\",\n",
        "                                  \"% 0\", \"% -1 to -.75\", \"% -.75 to -.5\", \"% -.5 to -.25\", \"% -.25 to 0\",\n",
        "                                  \"% 0 to .25\", \"% .25 to .5\", \"% .5 to .75\", \"%.75 to 1\",\n",
        "                                  \"%uv 0\", \"%uv -1 to -.75\", \"%uv -.75 to -.5\", \"%uv -.5 to -.25\", \"%uv -.25 to 0\",\n",
        "                                  \"%uv 0 to .25\", \"%uv .25 to .5\", \"%uv .5 to .75\", \"%uv .75 to 1\",\n",
        "                                  \"#votes 0\", \"#votes -1 to -.75\", \"#votes -.75 to -.5\", \"#votes -.5 to -.25\", \"#votes -.25 to 0\",\n",
        "                                  \"#votes 0 to .25\", \"#votes .25 to .5\", \"#votes .5 to .75\", \"#votes .75 to 1\",\n",
        "                                  \"#votes uv 0\", \"#votes uv -1 to -.75\", \"#votes uv -.75 to -.5\", \"#votes uv -.5 to -.25\", \"#votes uv -.25 to 0\",\n",
        "                                  \"#votes uv 0 to .25\", \"#votes uv .25 to .5\", \"#votes uv .5 to .75\", \"#votes uv .75 to 1\",\n",
        "                                  \"compound average summ\", \"compound SD summ\",\n",
        "                                  \"avg length of review\", \"avg length of summary\",\n",
        "                                  \"std length of review\", \"std length of summary\",\n",
        "                                  \"reviews\", \"summaries\", \"asin\"],\n",
        "                         index = range(unique))\n",
        "  #initialize variables\n",
        "  uniqueItems = 0\n",
        "  itemcount = 0\n",
        "  curritem = df['asin'].iloc[0]\n",
        "  votes = []\n",
        "  unixReview = []\n",
        "  reviewCount = 0\n",
        "  nones = 0\n",
        "  verified = []\n",
        "\n",
        "  #sentimental anaylsis variables\n",
        "  compound = []\n",
        "  compoundsumm = []\n",
        "  reviewlens = []\n",
        "  summarylens = []\n",
        "  reviews = []\n",
        "  summaries = []\n",
        "\n",
        "  #loop through every row in data\n",
        "  for i in range(df.shape[0]):\n",
        "    if (df['asin'].iloc[i] != curritem):\n",
        "      itemcount=itemcount+1\n",
        "\n",
        "      #append array into panda dataframe\n",
        "      newData.loc[uniqueItems] = parseRow(votes, unixReview, reviewCount, nones,\n",
        "                                          verified, compound, compoundsumm, reviewlens, summarylens,\n",
        "                                          reviews, summaries, df['asin'].iloc[i-1])\n",
        "\n",
        "      #reset the value of the variables\n",
        "      nones = 0\n",
        "      reviewCount = 0\n",
        "      votes = []\n",
        "      unixReview = []\n",
        "      verified = []\n",
        "      compound = []\n",
        "      compoundsumm = []\n",
        "      reviewlens = []\n",
        "      summarylens = []\n",
        "      reviews = []\n",
        "      summaries = []\n",
        "      uniqueItems = uniqueItems + 1\n",
        "      # end of if statement\n",
        "    curritem = df['asin'].iloc[i]\n",
        "    reviewCount = reviewCount + 1\n",
        "\n",
        "    #Percent of Nones counting number of nums\n",
        "    if df['vote'].iloc[i] == None:\n",
        "      nones += 1\n",
        "      votes.append(0)\n",
        "    else:\n",
        "      #Votes\n",
        "      numVotes = int(re.sub(\",\", \"\", df['vote'].iloc[i]))\n",
        "      votes.append(numVotes)\n",
        "\n",
        "    #Count verified\n",
        "    if df['verified'].iloc[i] == True:\n",
        "      verified.append(1)\n",
        "    else:\n",
        "      verified.append(0)\n",
        "\n",
        "    #append unixReviewTime\n",
        "    unixReview.append(int(df['unixReviewTime'].iloc[i]))\n",
        "\n",
        "    #sentimental analysis\n",
        "    compound.append(SA[0][i])\n",
        "    compoundsumm.append(SA[1][i])\n",
        "\n",
        "    #length of review and summary\n",
        "    if df['reviewText'].iloc[i] != None:\n",
        "      reviewlens.append(len(df['reviewText'].iloc[i]))\n",
        "    else:\n",
        "      reviewlens.append(0)\n",
        "\n",
        "    if df['summary'].iloc[i] != None:\n",
        "      summarylens.append(len(df['summary'].iloc[i]))\n",
        "    else:\n",
        "      summarylens.append(0)\n",
        "\n",
        "    #review and summary raw text\n",
        "    reviews.append(df['reviewText'].iloc[i])\n",
        "    summaries.append(df['summary'].iloc[i])\n",
        "\n",
        "    #end of for loop\n",
        "\n",
        "  # parse last rows data\n",
        "  newData.loc[unique - 1] = parseRow(votes, unixReview, reviewCount, nones,\n",
        "                                     verified, compound, compoundsumm, reviewlens, summarylens,\n",
        "                                     reviews, summaries, df['asin'].iloc[unique-1])\n",
        "\n",
        "  return newData\n",
        "\n",
        "def standardize(df):\n",
        "  result = df.copy()\n",
        "  for feature_name in df.columns:\n",
        "    if feature_name not in [\"reviews\", \"summaries\", \"asin\"] and feature_name[0] != '%':\n",
        "      #convert column to numeric\n",
        "      result[feature_name] = pd.to_numeric(df[feature_name])\n",
        "\n",
        "      #this line is the calculation\n",
        "      if result[feature_name].std() != 0 and result[feature_name].max() != 0:\n",
        "\n",
        "        result[feature_name] = (result[feature_name]-result[feature_name].mean()) / result[feature_name].std()\n",
        "        #make 0 lowest value\n",
        "        result[feature_name] = result[feature_name] + abs(result[feature_name].min())\n",
        "        #standardize to 0-1\n",
        "        result[feature_name] = result[feature_name]/result[feature_name].max()\n",
        "        print(\"standardized column \", feature_name)\n",
        "  return result\n",
        "\n",
        "dfsmall = df[:2000]\n",
        "unique = uniqueItems(df)\n",
        "trainDataSA = sentimentalAnalysis(df)\n",
        "trainData = parseData(unique, trainDataSA, df)\n",
        "\n",
        "trainDataStandardized = standardize(trainData)\n",
        "\n",
        "print(trainData.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainDataStandardized.iloc[1])"
      ],
      "metadata": {
        "id": "xwkxRGylPQkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VQwx9ylZhLY"
      },
      "outputs": [],
      "source": [
        "def standardize(df):\n",
        "  result = df.copy()\n",
        "  for feature_name in df.columns:\n",
        "    #convert column to numeric\n",
        "    result[feature_name] = pd.to_numeric(df[feature_name])\n",
        "\n",
        "    #standardize data\n",
        "    max_value = result[feature_name].max()\n",
        "    min_value = result[feature_name].min()\n",
        "    #this line is the calculation\n",
        "    result[feature_name] = (result[feature_name]-result[feature_name].mean()) / result[feature_name].std()\n",
        "    result[feature_name] = result[feature_name] + result[feature_name].min()\n",
        "  return result\n",
        "\n",
        "trainData = trainData.iloc[:,:30]\n",
        "trainData = standardize(trainData)\n",
        "trainData.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxnTnWZBMh_J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ab03abf6-cbf9-4a0a-d728-56dbbc65b992"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_580097b2-e953-4f8e-b4db-c614c1772076\", \"vaderHistogramStandardizedTest.json\", 173432728)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Download data\n",
        "with open('vaderHistogramStandardizedTest.json', 'w') as f:\n",
        "  f.write(trainDataStandardized.to_json())\n",
        "files.download('vaderHistogramStandardizedTest.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectPercentile, chi2\n",
        "#for i in range(10):\n",
        "tfidf = TfidfVectorizer(stop_words ='english')\n",
        "X_train, X_test, y_train, y_test = train_test_split(trainData, awesometrain, test_size=0.25)\n",
        "tog = X_train['reviews'] + X_train['summaries']\n",
        "togtest = X_test['reviews'] + X_test['summaries']\n",
        "\n",
        "transf = tfidf.fit_transform(tog.fillna(\"\"))\n",
        "transf = pd.DataFrame.sparse.from_spmatrix(transf)\n",
        "transftest = tfidf.transform(togtest.fillna(\"\"))\n",
        "tramsftest = pd.DataFrame.sparse.from_spmatrix(transftest)\n",
        "\n",
        "selection = SelectPercentile(chi2, percentile=12)\n",
        "trainDataTFIDF = selection.fit_transform(transf, y_train)\n",
        "trainDataTFIDFTest = selection.transform(transftest)\n",
        "\n",
        "trainDataTFIDF = pd.DataFrame.sparse.from_spmatrix(trainDataTFIDF)\n",
        "trainDataTFIDFTest = pd.DataFrame.sparse.from_spmatrix(trainDataTFIDFTest)"
      ],
      "metadata": {
        "id": "aEYeQEGjPIBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "X_trainsparse = scipy.sparse.csr_matrix(X_train.iloc[:,:54].values)\n",
        "trainDataTFIDF = hstack((X_trainsparse, trainDataTFIDF))"
      ],
      "metadata": {
        "id": "zKPKxCHeXoU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_trainsparse)"
      ],
      "metadata": {
        "id": "GhT1P6RyYWUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse\n",
        "from scipy.sparse import hstack\n",
        "#dtype = pd.SparseDtype(float, fill_value=0)\n",
        "#col = X_train.iloc[:,:54].astype(dtype)\n",
        "\n",
        "\n",
        "for column in X_train.iloc[:,:54].columns:\n",
        "  trainDataTFIDF = trainDataTFIDF.drop([column], axis=1)\n",
        "  trainDataTFIDF[column] = X_train[column]\n",
        "  print('appended', column)\n",
        "\n",
        "for rfw in range(0, 5):\n",
        "  for dtc in range(0, 5):\n",
        "    for threshold in range(.4, .65, .05):\n",
        "      sum = rfw+dtc\n",
        "      predweight = (rfw*predsrfw + dtc*predsdtc)/sum\n",
        "      if predweight > threshold:\n",
        "        class 1\n",
        "      else\n",
        "        class 2"
      ],
      "metadata": {
        "id": "bho7uP_hQry_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainDataTFIDF = trainDataTFIDF.fillna(0)\n",
        "print(X_train.iloc[2])\n",
        "print(trainDataTFIDF.iloc[2])"
      ],
      "metadata": {
        "id": "e0yyDokya9N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = X_test.iloc[:,:54].astype(dtype)\n",
        "train = hstack((col, trainDataTFIDFtest))"
      ],
      "metadata": {
        "id": "6q5DRZ0ET54d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79wPVmRxvutZ"
      },
      "outputs": [],
      "source": [
        "#feature selection function\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# takes in a classifier, training data, and the classes and returns the\n",
        "# ranked best indices of the training data\n",
        "def featureSelection(clf, trainData, awesometrain):\n",
        "  #initialize variables\n",
        "  columns = trainData.columns\n",
        "  scores = np.zeros(trainData.shape[1])\n",
        "\n",
        "\n",
        "  #baseline KNN\n",
        "  baseLine = np.average(cross_val_score(clf, trainData, awesometrain, scoring='f1',\n",
        "                                        cv=10))\n",
        "  # loop through all columns\n",
        "  for i in range(trainData.shape[1]):\n",
        "\n",
        "    # calculate score of column\n",
        "    scores[i] = -1 * (np.average(cross_val_score(clf, trainData.iloc[:, trainData.columns != columns[i]],\n",
        "                           awesometrain, scoring='f1', cv=10)) / baseLine - 1)\n",
        "    #print(\"Column Name: %s, f1-score: %.3f\" % (columns[i], scores[i]))\n",
        "    # end of for\n",
        "\n",
        "    #Find most and least impactful features\n",
        "  Indices = np.argsort(scores)\n",
        "  '''\n",
        "  print(\"Top 5 impactful features:\")\n",
        "  for i in range(5):\n",
        "    print(\"Feature: %s: %f\" % (columns[Indices[-1*i - 1]], scores[Indices[-1*i - 1]]))\n",
        "\n",
        "  print(\"Bottom 5 impactful features:\")\n",
        "  for i in range(5):\n",
        "    print(\"Feature: %s: %f\" % (columns[Indices[i]], scores[Indices[i]]))\n",
        "\n",
        "  # plot results\n",
        "  plt.figure(figsize=(20,10))\n",
        "\n",
        "  # creating the bar plot\n",
        "  plt.bar(range(len(columns)), scores, color ='maroon',\n",
        "          width = 1)\n",
        "  plt.xlabel(\"Feature index\")\n",
        "  plt.ylabel(\"Percent worse\")\n",
        "  plt.title(\"Feature Selection\")\n",
        "  plt.show()\n",
        "  '''\n",
        "  return Indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMQt0fYvY66M"
      },
      "outputs": [],
      "source": [
        "# recursive Feature Select\n",
        "# takes in a classifer, the dataframe and a datafram of the class\n",
        "# outputs the best f1-score from recursing through features greedily\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def RFS(clf, trainData, awesomeTrain, visited={}, startF1 = 0):\n",
        "\n",
        "  #initialize variables\n",
        "  columns = trainData.columns\n",
        "  bestF1 = 0\n",
        "\n",
        "  #baseline KNN\n",
        "  baseLine = np.average(cross_val_score(clf, trainData, awesometrain, scoring='f1',\n",
        "                                        cv=10))\n",
        "  # loop through all columns\n",
        "  for i in range(trainData.shape[1]):\n",
        "    # if we remove a column and it is empty, then we have gone to far\n",
        "    if trainData.iloc[:, trainData.columns != columns[i]].empty: return [0, visited]\n",
        "\n",
        "    #initialize variable\n",
        "    listx = columns[columns != columns[i]]\n",
        "\n",
        "    # calculate score of column\n",
        "    if visited.get(frozenset(listx)):\n",
        "      return [visited.get(frozenset(listx)), visited]\n",
        "    else:\n",
        "      x_train, x_test, y_train, y_test = train_test_split(trainData.iloc[:, trainData.columns != columns[i]],\n",
        "                                                          awesometrain, test_size = 0.1)\n",
        "      clf.fit(x_train, y_train)\n",
        "      F1 = f1_score(y_test, clf.predict(x_test))\n",
        "      score = -1 * (F1 / baseLine - 1)\n",
        "      visited[frozenset(listx)] = F1\n",
        "\n",
        "    bestF1 = max(bestF1, F1, startF1)\n",
        "    print(bestF1)\n",
        "\n",
        "    # if score is negative, throw out the column\n",
        "    if score < 0:\n",
        "      temp, visited = RFS(clf, trainData.iloc[:, trainData.columns != columns[i]], awesometrain, visited=visited, startF1=bestF1)\n",
        "      bestF1 = max(bestF1, temp)\n",
        "    else:\n",
        "      bestF1 = max(bestF1, F1)\n",
        "\n",
        "    #end function\n",
        "  return [bestF1, visited]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA feature reduction\n",
        "trainDataCP = trainData.copy()\n",
        "\n",
        "pca = PCA(n_components=10)\n",
        "trainDataCP = pca.fit_transform(trainDataCP)\n",
        "trainDataCP = pd.DataFrame(trainDataCP)"
      ],
      "metadata": {
        "id": "vqPI79h6zqTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK1bK-i4FfgS"
      },
      "outputs": [],
      "source": [
        "#Decision Trees\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "absBestOutput = \"\"\n",
        "absBestScore = 0\n",
        "\n",
        "for n in range(3, 30, 3):\n",
        "  bestOutput = \"\"\n",
        "  bestScore = 0\n",
        "  trainDataCP = trainData.copy()\n",
        "  pca = PCA(n_components=n)\n",
        "  trainDataCP = pca.fit_transform(trainDataCP)\n",
        "  trainDataCP = pd.DataFrame(trainDataCP)\n",
        "  for c in ['entropy', 'gini', 'log_loss']:\n",
        "    for s in ['best', 'random']:\n",
        "      for i in range(1, math.floor(n/2)+5):\n",
        "        clf = DecisionTreeClassifier(criterion=c, splitter=s, max_depth = i)\n",
        "\n",
        "        scores = cross_val_score(clf, trainDataCP, awesometrain, scoring='f1_weighted', cv=10)\n",
        "        output = 'pca folds: %d, criterion: %s, splitter: %s, max_depth: %d, F1: %f, F1std: %f' % (n, c, s, i, np.average(scores), np.std(scores))\n",
        "        if np.average(scores)-np.std(scores) > bestScore:\n",
        "          bestOutput = output\n",
        "          bestScore = np.average(scores)-np.std(scores)\n",
        "          print('new best:', output)\n",
        "        print(output)\n",
        "  print('The best option for decision tree is: %s' % (bestOutput))\n",
        "  if(bestScore > absBestScore):\n",
        "    absBestOutput = bestOutput\n",
        "    absBestScore = bestScore\n",
        "    print('new best in general, ', absBestOutput)\n",
        "\n",
        "print('The best option in general is %s' % (absBestOutput))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bestOutput = \"\"\n",
        "bestScore = 0\n",
        "n = 0\n",
        "for c in ['entropy', 'gini', 'log_loss']:\n",
        "  for s in ['best', 'random']:\n",
        "    for i in range(3, 16):\n",
        "      clf = DecisionTreeClassifier(criterion=c, splitter=s, max_depth = i)\n",
        "\n",
        "      scores = cross_val_score(clf, trainDataCP, awesometrain, scoring='f1_weighted', cv=5)\n",
        "      output = 'pca folds: %d, criterion: %s, splitter: %s, max_depth: %d, F1: %f, F1std: %f' % (n, c, s, i, np.average(scores), np.std(scores))\n",
        "      if np.average(scores)-np.std(scores) > bestScore:\n",
        "        bestOutput = output\n",
        "        bestScore = np.average(scores)-np.std(scores)\n",
        "      print(output)\n",
        "print('The best option for decision tree is: %s' % (bestOutput))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NW7KY8kXDMz",
        "outputId": "02616fcd-88d9-40b8-9f1e-ff5307dd4c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 3, F1: 0.565771, F1std: 0.004125\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 4, F1: 0.572591, F1std: 0.005799\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 5, F1: 0.575635, F1std: 0.003551\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 6, F1: 0.576188, F1std: 0.004706\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 7, F1: 0.580451, F1std: 0.007199\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 8, F1: 0.584081, F1std: 0.004517\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 9, F1: 0.586092, F1std: 0.003093\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 10, F1: 0.584341, F1std: 0.004663\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 11, F1: 0.582012, F1std: 0.004210\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 12, F1: 0.575474, F1std: 0.005069\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 13, F1: 0.576116, F1std: 0.004160\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 14, F1: 0.574031, F1std: 0.004492\n",
            "pca folds: 0, criterion: entropy, splitter: best, max_depth: 15, F1: 0.569036, F1std: 0.005325\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 3, F1: 0.562992, F1std: 0.007680\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 4, F1: 0.548638, F1std: 0.017535\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 5, F1: 0.565768, F1std: 0.015921\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 6, F1: 0.567983, F1std: 0.013801\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 7, F1: 0.570880, F1std: 0.007976\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 8, F1: 0.585652, F1std: 0.003446\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 9, F1: 0.582005, F1std: 0.003736\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 10, F1: 0.582534, F1std: 0.005308\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 11, F1: 0.578973, F1std: 0.001962\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 12, F1: 0.579241, F1std: 0.002737\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 13, F1: 0.580523, F1std: 0.004076\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 14, F1: 0.577839, F1std: 0.003315\n",
            "pca folds: 0, criterion: entropy, splitter: random, max_depth: 15, F1: 0.573687, F1std: 0.003319\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 3, F1: 0.565663, F1std: 0.004040\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 4, F1: 0.572343, F1std: 0.005502\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 5, F1: 0.574561, F1std: 0.003298\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 6, F1: 0.575027, F1std: 0.003869\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 7, F1: 0.581759, F1std: 0.006221\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 8, F1: 0.580699, F1std: 0.004950\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 9, F1: 0.579463, F1std: 0.003811\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 10, F1: 0.578049, F1std: 0.006759\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 11, F1: 0.580008, F1std: 0.005018\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 12, F1: 0.576385, F1std: 0.005922\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 13, F1: 0.573525, F1std: 0.005448\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 14, F1: 0.571982, F1std: 0.004743\n",
            "pca folds: 0, criterion: gini, splitter: best, max_depth: 15, F1: 0.568904, F1std: 0.003970\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 3, F1: 0.547205, F1std: 0.027473\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 4, F1: 0.537039, F1std: 0.021834\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 5, F1: 0.571553, F1std: 0.008073\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 6, F1: 0.578712, F1std: 0.004106\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 7, F1: 0.575392, F1std: 0.006670\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 8, F1: 0.576841, F1std: 0.003723\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 9, F1: 0.577669, F1std: 0.002005\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 10, F1: 0.580816, F1std: 0.005777\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 11, F1: 0.579193, F1std: 0.002772\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 12, F1: 0.577224, F1std: 0.002377\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 13, F1: 0.579701, F1std: 0.001902\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 14, F1: 0.576926, F1std: 0.005028\n",
            "pca folds: 0, criterion: gini, splitter: random, max_depth: 15, F1: 0.573084, F1std: 0.001564\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 3, F1: 0.565771, F1std: 0.004125\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 4, F1: 0.572591, F1std: 0.005799\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 5, F1: 0.575635, F1std: 0.003551\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 6, F1: 0.576188, F1std: 0.004706\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 7, F1: 0.580372, F1std: 0.007106\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 8, F1: 0.584159, F1std: 0.004451\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 9, F1: 0.586086, F1std: 0.003307\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 10, F1: 0.584280, F1std: 0.004814\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 11, F1: 0.581474, F1std: 0.004062\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 12, F1: 0.575470, F1std: 0.005954\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 13, F1: 0.575979, F1std: 0.003655\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 14, F1: 0.573493, F1std: 0.004244\n",
            "pca folds: 0, criterion: log_loss, splitter: best, max_depth: 15, F1: 0.569568, F1std: 0.005958\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 3, F1: 0.539438, F1std: 0.040652\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 4, F1: 0.562742, F1std: 0.014332\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 5, F1: 0.568346, F1std: 0.016614\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 6, F1: 0.569872, F1std: 0.016388\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 7, F1: 0.580902, F1std: 0.005909\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 8, F1: 0.573479, F1std: 0.006511\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 9, F1: 0.575298, F1std: 0.002474\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 10, F1: 0.580830, F1std: 0.004748\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 11, F1: 0.582156, F1std: 0.003898\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 12, F1: 0.577881, F1std: 0.003607\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 13, F1: 0.580264, F1std: 0.001892\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 14, F1: 0.577765, F1std: 0.002932\n",
            "pca folds: 0, criterion: log_loss, splitter: random, max_depth: 15, F1: 0.574243, F1std: 0.004618\n",
            "The best option for decision tree is: pca folds: 0, criterion: entropy, splitter: best, max_depth: 9, F1: 0.586092, F1std: 0.003093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "n = 12\n",
        "c = 'log_loss'\n",
        "s = 'best'\n",
        "i = 6\n",
        "averages = []\n",
        "for x in range(5):\n",
        "  trainDataCP = trainData.copy()\n",
        "  pca = PCA(n_components=n)\n",
        "  trainDataCP = pca.fit_transform(trainDataCP)\n",
        "  trainDataCP = pd.DataFrame(trainDataCP)\n",
        "  clf = DecisionTreeClassifier(criterion=c, splitter=s, max_depth = i)\n",
        "\n",
        "  scores = cross_val_score(clf, trainDataCP, awesometrain, scoring='f1', cv=10)\n",
        "  averages.append(np.average(scores))\n",
        "  print('pca folds: %d, criterion: %s, splitter: %s, max_depth: %d, F1: %f' % (n, c, s, i, np.average(scores)))\n",
        "\n",
        "print('average score: %f' % np.average(averages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q38Aq7G585a4",
        "outputId": "07b9cfa6-21bf-4948-f79e-ac4406bacb2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pca folds: 12, criterion: log_loss, splitter: best, max_depth: 6, F1: 0.636327\n",
            "pca folds: 12, criterion: log_loss, splitter: best, max_depth: 6, F1: 0.635835\n",
            "pca folds: 12, criterion: log_loss, splitter: best, max_depth: 6, F1: 0.635261\n",
            "pca folds: 12, criterion: log_loss, splitter: best, max_depth: 6, F1: 0.635307\n",
            "pca folds: 12, criterion: log_loss, splitter: best, max_depth: 6, F1: 0.636601\n",
            "average score: 0.635866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "bestOutput = \"\"\n",
        "bestScore = 0\n",
        "n = 0\n",
        "for c in ['entropy', 'gini', 'log_loss']:\n",
        "  for i in range(7, 15):\n",
        "    clf = RandomForestClassifier(criterion=c, max_depth=i, n_jobs=-1, verbose=0)\n",
        "\n",
        "    scores = cross_val_score(clf, trainData, awesometrain, scoring='f1_weighted', cv=10)\n",
        "    output = 'criterion: %s, max_depth: %d, F1: %f, F1std: %f' % (c, i, np.average(scores), np.std(scores))\n",
        "    if np.average(scores)-np.std(scores) > bestScore:\n",
        "      bestOutput = output\n",
        "      bestScore = np.average(scores)-np.std(scores)\n",
        "    print(output)\n",
        "print('The best option for random forest is: %s' % (bestOutput))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rAhzyS6PQrw",
        "outputId": "9f96cbcd-9886-419a-bea2-f710392abab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "criterion: entropy, max_depth: 7, F1: 0.590241, F1std: 0.004109\n",
            "criterion: entropy, max_depth: 8, F1: 0.595091, F1std: 0.004941\n",
            "criterion: entropy, max_depth: 9, F1: 0.597115, F1std: 0.004398\n",
            "criterion: entropy, max_depth: 10, F1: 0.599393, F1std: 0.003988\n",
            "criterion: entropy, max_depth: 11, F1: 0.601197, F1std: 0.004324\n",
            "criterion: entropy, max_depth: 12, F1: 0.602585, F1std: 0.004626\n",
            "criterion: entropy, max_depth: 13, F1: 0.603946, F1std: 0.005421\n",
            "criterion: entropy, max_depth: 14, F1: 0.603287, F1std: 0.004111\n",
            "criterion: gini, max_depth: 7, F1: 0.591253, F1std: 0.003647\n",
            "criterion: gini, max_depth: 8, F1: 0.595340, F1std: 0.004301\n",
            "criterion: gini, max_depth: 9, F1: 0.596856, F1std: 0.003987\n",
            "criterion: gini, max_depth: 10, F1: 0.598743, F1std: 0.003958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8WAIdrq-0eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulMuJz_Qqmsg"
      },
      "outputs": [],
      "source": [
        "# Decision Tree feature select\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth = 2)\n",
        "score, dictionary = RFS(clf, trainData, awesometrain)\n",
        "\n",
        "print(\"best combination of features for decision tree\")\n",
        "for i in dictionary:\n",
        "  if(abs(score - dictionary.get(i)) < 0.0001):\n",
        "    print(i)\n",
        "    print(dictionary.get(i))\n",
        "    break;\n",
        "\n",
        "#0.7105"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRK6vWQKnArC",
        "outputId": "5cf6633d-a123-425f-f522-7584aa2c3023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 549 2851]\n",
            " [ 412 3497]]\n",
            "0.6818757921419518\n"
          ]
        }
      ],
      "source": [
        "# Decision Tree confusion matrix + F1 score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(trainData, awesometrain, test_size=0.1)\n",
        "clf = DecisionTreeClassifier(criterion='entropy', splitter='random', max_depth = 2)\n",
        "clf.fit(x_train, y_train)\n",
        "predict = clf.predict(x_test)\n",
        "print(confusion_matrix(y_test, predict))\n",
        "print(f1_score(y_test, predict))\n",
        "# Confusion: [[ 549 2851]\n",
        "#             [ 412 3497]]\n",
        "# f1: 0.6818757921419518"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyiqBWCo_qBZ"
      },
      "outputs": [],
      "source": [
        "#KNN Feature Selection\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#initialize variables\n",
        "columns = trainData.columns\n",
        "scores = np.zeros(trainData.shape[1])\n",
        "clf = KNeighborsClassifier(n_neighbors=100, n_jobs=-1)\n",
        "\n",
        "featureSelection(clf, trainData, awesometrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HcaMDAB2kVr"
      },
      "outputs": [],
      "source": [
        "#KNN hyperparameter tuning\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "bestscore = 0\n",
        "bestoutput = \"\"\n",
        "for i in range(50, 151, 50):\n",
        "  for j in ['uniform', 'distance']:\n",
        "    for k in ['auto', 'ball_tree', 'kd_tree', 'brute']:\n",
        "      for l in range(15, 46, 15):\n",
        "        clf = KNeighborsClassifier(n_neighbors=i, weights = j, algorithm=k,\n",
        "                                   leaf_size=l, n_jobs=-1)\n",
        "        x_train, x_test, y_train, y_test = train_test_split(trainData, awesometrain, test_size=0.1)\n",
        "        clf.fit(x_train, y_train)\n",
        "        scores = f1_score(y_test, clf.predict(x_test))\n",
        "        #scores = cross_val_score(clf, trainData, awesometrain, scoring='f1', cv=10)\n",
        "        avgscore = np.average(scores)\n",
        "        output = \"neightbors: %d, weights: %s, algorithm: %s, leaf_size: %d, F1: %f\" % (i, j, k, l, np.average(avgscore))\n",
        "        if avgscore > bestscore:\n",
        "          bestscore = avgscore\n",
        "          bestoutput = output\n",
        "        print(output)\n",
        "print('The best option for KNN is: ', bestoutput)\n",
        "# best option\n",
        "# The best option for KNN is:  neightbors: 100, weights: distance, algorithm: auto, leaf_size: 15, F1: 0.668424"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j4_Wr_gC2Ox"
      },
      "outputs": [],
      "source": [
        "# KNN Hyperparameter tuning+feature Selection\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=100, weights = 'distance', algorithm='auto', leaf_size=15, n_jobs=-1)\n",
        "\n",
        "print(\"Baseline: %f\" % (np.average(cross_val_score(clf, trainData,\n",
        "                                 awesometrain, scoring='f1', cv=10))))\n",
        "print(\"feature_selection: %f\" % (np.average(cross_val_score(clf, trainData.iloc[:, [0,2,3,4,6,8,9,11,16,17]],\n",
        "                                 awesometrain, scoring='f1', cv=10))))\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0BJfi0T_5N3"
      },
      "outputs": [],
      "source": [
        "#Naive Bayes Feature Selection\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#initialize variables\n",
        "columns = trainData.columns\n",
        "scores = np.zeros(trainData.shape[1])\n",
        "clf = BernoulliNB()\n",
        "\n",
        "featureSelection(clf, trainData, awesometrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_uyIqO1EXyh"
      },
      "outputs": [],
      "source": [
        "#Naive Bayes hyperparameter tuning\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "bestscore = 0\n",
        "bestoutput = \"\"\n",
        "algo = [GaussianNB(), BernoulliNB(force_alpha=True)]\n",
        "name = ['GaussianNB()', 'BernoulliNB()']\n",
        "for i in range(2):\n",
        "  #calculate score\n",
        "  score = np.average(cross_val_score(algo[i], trainData, awesometrain, scoring='f1', cv=10))\n",
        "\n",
        "  #update output\n",
        "  output = \"Algorithm: %s, f1 score: %.3f\" % (name[i], score)\n",
        "\n",
        "  #update best score and output\n",
        "  if score > bestscore:\n",
        "    bestscore = score\n",
        "    bestoutput = output\n",
        "\n",
        "  print(output)\n",
        "\n",
        "print('The best option for NB is: ', bestoutput)\n",
        "# best option for NB is BernoulliNB, f1 score: 0.600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYcSYFC3IJyn"
      },
      "outputs": [],
      "source": [
        "#NB feature selection + hyper parameter tuning; f1-score: 62%\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#initialize variables\n",
        "algo = [GaussianNB(), BernoulliNB(force_alpha=True)]\n",
        "name = ['GaussianNB():', 'BernoulliNB():']\n",
        "\n",
        "\n",
        "for i in range(2):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(trainData, awesometrain)\n",
        "  clf = algo[i]\n",
        "  clf.fit(x_train, y_train)\n",
        "  predict=clf.predict(x_test)\n",
        "  print(name[i])\n",
        "  print(\"Recall: %f\" % (metrics.recall_score(y_test, predict)))\n",
        "  print(\"Precision: %f\" % (metrics.precision_score(y_test, predict)))\n",
        "  print(\"f1: %f\" % (f1_score(y_test, predict)))\n",
        "\n",
        "  '''\n",
        "  clf = algo[1]\n",
        "  score, visited = RFS(clf, trainData, awesometrain, {})\n",
        "  print(\"Baseline: %s, f1-score: %f\" % (name[1], np.average(cross_val_score(clf, trainData,\n",
        "                                 awesometrain, scoring='f1', cv=10))))\n",
        "  print(\"feature_selection: %s, f1-score: %f\" % (name[1], score))\n",
        "'''\n",
        "# Baseline: GaussianNB(), f1-score: 0.354134\n",
        "# feature_selection: GaussianNB(), f1-score: 0.675947\n",
        "# Baseline: BernoulliNB, f1-score: f1-score: 0.597\n",
        "# feature_selection: BernoulliNB: f1-score: 0.691"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC2pP8uqW2PA"
      },
      "outputs": [],
      "source": [
        "#Random Forest Classifier\n",
        "#feature selection\n",
        "#create classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "clasf = RandomForestClassifier(criterion='log_loss', verbose = 2)\n",
        "cv = StratifiedKFold(n_splits = 10)\n",
        "f = RFECV(estimator = clasf, step = 1, cv=cv, scoring = 'f1', n_jobs =-1)\n",
        "features = f.fit(trainData, awesometrain)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "#train new random forest with tuned parameters\n",
        "clasfwithhyper = RandomForestClassifier(criterion = 'entropy', max_features = 3, min_samples_leaf = 5, min_samples_split = 10)\n",
        "rfnew = RFECV(clasfwithhyper, step = 1, cv = StratifiedKFold(n_splits=10), scoring = 'f1', n_jobs=  -1)\n",
        "newfeat = rfnew.fit(trainData, awesometrain)\n",
        "\n",
        "#6\n",
        "from sklearn import metrics\n",
        "y_predlog = newfeat.predict(trainData)\n",
        "cm = metrics.confusion_matrix(awesometrain, y_predlog)\n",
        "cm\n",
        "f1 = np.average(cross_val_score(rfnew, trainData, awesometrain, scoring = 'f1', cv=10))\n",
        "precision = np.average(cross_val_score(rfnew, trainData,awesometrain, scoring = 'precision', cv=10))\n",
        "recall = np.average(cross_val_score(rfnew, trainData, awesometrain, scoring = 'recall', cv=10))\n",
        "print(f1, precision, recall)\n",
        "print(cm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJYp68kNbjMn"
      },
      "outputs": [],
      "source": [
        "#SVM Hyperparameter Tuning\n",
        "#max_iter=-1: F1 .668\n",
        "bestOutput = \"\"\n",
        "bestScore = 0\n",
        "for k in ['rbf']:\n",
        "  for i in range(1):\n",
        "    for mi in range(-1):\n",
        "      clf = svm.SVC(kernel = 'rbf', C=1, max_iter=-1)\n",
        "\n",
        "      scores = cross_val_score(clf, trainData, awesometrain, scoring='f1', cv=2)\n",
        "      output = 'kernel: %s, C: %d, max_iter: %d, F1: %f' % (k, i, mi, np.average(scores))\n",
        "\n",
        "      if np.average(scores) > bestScore:\n",
        "        bestOutput = output\n",
        "        bestScore = np.average(scores)\n",
        "\n",
        "      print(output)\n",
        "print('The best option for svm is: ', bestOutput)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtCciTbjghI5"
      },
      "outputs": [],
      "source": [
        "#Late fusion with KNN, Logistic regression, decision tree, random forst, Bernoulli Naive Bayes\n",
        "# F1-score: NB: 0.694, KNN: 0.651, LogReg: 0.660, decisionTree: 0.662, randomForest: 0.663\n",
        "# with trainData: NB: 0.600, KNN: 0.654, LogReg: 0.660, decisionTree: 0.663, randomForest: 0.659\n",
        "# with feature selection: NB: , KNN: , Logreg: , decisionTree: , randomForest:\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import metrics\n",
        "\n",
        "#initialize classifiers\n",
        "NB = BernoulliNB()\n",
        "KNN = KNeighborsClassifier(n_neighbors=100, leaf_size=15, weights='distance', algorithm='auto')\n",
        "logReg = LogisticRegressionCV(cv=3, refit = False, solver = 'saga')\n",
        "decisionTree = DecisionTreeClassifier(criterion=\"gini\", splitter='best', max_depth = 3)\n",
        "randomForest = RandomForestClassifier(criterion='log_loss', min_samples_split = 5, min_samples_leaf = 2)\n",
        "\n",
        "#create data\n",
        "x_train, x_test, y_train, y_test = train_test_split(trainData,awesometrain,test_size = 0.1)\n",
        "\n",
        "# fit classifiers to data\n",
        "NB.fit(x_train, y_train)\n",
        "KNN.fit(x_train, y_train)\n",
        "logReg.fit(x_train, y_train)\n",
        "decisionTree.fit(x_train, y_train)\n",
        "randomForest.fit(x_train, y_train)\n",
        "\n",
        "#initialize panda datafram\n",
        "lateFusion = pd.DataFrame()\n",
        "lateFusionTest = pd.DataFrame()\n",
        "\n",
        "# get columns and append to pandas dataframe\n",
        "lateFusion[\"Naive Bayes\"] = NB.predict_proba(x_train)[:,1]\n",
        "lateFusion[\"KNN\"] = KNN.predict_proba(x_train)[:,1]\n",
        "lateFusion[\"logReg\"] = logReg.predict_proba(x_train)[:,1]\n",
        "lateFusion[\"decisionTree\"] = decisionTree.predict_proba(x_train)[:,1]\n",
        "lateFusion[\"randomForest\"] = randomForest.predict_proba(x_train)[:,1]\n",
        "\n",
        "lateFusionTest[\"Naive Bayes\"] = NB.predict_proba(x_test)[:,1]\n",
        "lateFusionTest[\"KNN\"] = KNN.predict_proba(x_test)[:,1]\n",
        "lateFusionTest[\"logReg\"] = logReg.predict_proba(x_test)[:,1]\n",
        "lateFusionTest[\"decisionTree\"] = decisionTree.predict_proba(x_test)[:,1]\n",
        "lateFusionTest[\"randomForest\"] = randomForest.predict_proba(x_test)[:,1]\n",
        "# combine previous data with lateFusion for more info\n",
        "#lateFusion.index = np.arange(1,len(lateFusion)+1)\n",
        "\n",
        "# f1-score no trainingdata just late fusion\n",
        "'''\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", splitter='best', max_depth = 3)\n",
        "clf.fit(lateFusion, y_train)\n",
        "predict = clf.predict(lateFusionTest)\n",
        "f1 = f1_score(y_test, predict)\n",
        "recall = metrics.recall_score(y_test, predict)\n",
        "precision = metrics.precision_score(y_test, predict)\n",
        "print(metrics.confusion_matrix(y_test, predict))\n",
        "print(f1, precision, recall)\n",
        "\n",
        "lateFusion = pd.concat([trainData, lateFusion], axis=1)\n",
        "\n",
        "# run late Fusion\n",
        "print(\"NB\")\n",
        "for i in range(len(lateFusion.columns)):\n",
        "  print(\"# of features: %d f1-score: %.3f\" % (i, np.average(cross_val_score(NB, lateFusion.iloc[:, featureSelection(NB, lateFusion, awesometrain)[:i+1]], awesometrain, scoring='f1', cv=10)), ))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJt6vNxJT1wy"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(trainData, awesometrain, test_size = 0.35)\n",
        "clf = LogisticRegressionCV(cv = 10)\n",
        "cv = StratifiedKFold(n_splits=10)\n",
        "rf = RFE(clf, n_features_to_select=8)\n",
        "features1 = rf.fit(X_train, Y_train)\n",
        "features1.support_\n",
        "store = pd.DataFrame({'columns':X_train.columns, 'Kept': features1.support_})\n",
        "new= X_train.iloc[:, features1.support_]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "vhwuyHGLUhlv",
        "outputId": "c98e6cf5-b23c-40a3-c6db-1d72569da949"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(cv=10, refit=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(cv=10, refit=False)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegressionCV(cv=10, refit=False)"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "logreg = {'solver':['lbfgs', 'liblinear','newton-cg','sag', 'saga','newton-cholesky'], 'refit':[True, False], 'penalty':['l2', 'elasticnet']}\n",
        "def gridsearch(model, params_grid):\n",
        "  search = GridSearchCV(estimator = clf, param_grid = params_grid, scoring = 'f1', cv = 3, n_jobs=-1)\n",
        "  return search.fit(new, Y_train)\n",
        "bestrf = gridsearch(clf, logreg).best_estimator_\n",
        "bestrf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "gT9i6vecUfwE",
        "outputId": "f1ecff5a-62cb-430d-f6f8-7fbe16cc4c4f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(cv=10, refit=False, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(cv=10, refit=False, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegressionCV(cv=10, refit=False, solver='saga')"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "best = LogisticRegressionCV(solver = 'saga', cv = 10, refit = False)\n",
        "best.fit(new, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vMU_ugNwUeMb",
        "outputId": "26d5d22b-92ff-4469-dfbd-4a8685c0178e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6522842591749229 0.5925076418830011 0.7255185786006113\n",
            "[[5088 6968]\n",
            " [3569 9954]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "y_predlog = best.predict(X_test.iloc[:, features1.support_])\n",
        "cm = metrics.confusion_matrix(Y_test, y_predlog)\n",
        "\n",
        "f1 = np.average(cross_val_score(best, trainData.iloc[:, features1.support_], awesometrain, scoring = 'f1', cv=10))\n",
        "precision = np.average(cross_val_score(best, trainData.iloc[:, features1.support_], awesometrain, scoring = 'precision', cv=10))\n",
        "recall = np.average(cross_val_score(best, trainData.iloc[:, features1.support_], awesometrain, scoring = 'recall', cv=10))\n",
        "print(f1, precision, recall)\n",
        "print(cm)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}